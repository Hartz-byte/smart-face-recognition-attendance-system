{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769ddad0-5a77-4e11-b1fb-8ce729b7f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243c6fcb-3b02-42f8-9c7e-3a662cb258db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752384950.683747    3917 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752384950.753253    3917 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752384951.340300    3917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752384951.340342    3917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752384951.340345    3917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752384951.340347    3917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae3e2bb-8379-4aa6-a392-e01e96719bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
    "FER_CSV_PATH = \"../emotions_csv/fer2013.csv\"\n",
    "MODEL_SAVE_PATH = \"saved_model/vit_emotion_head.pth\"\n",
    "EMOTION_CLASSES = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LR = 1e-4\n",
    "PATIENCE = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4291cbdf-0635-4750-aa0f-1adc9a5cb54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class FERDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        pixels = np.array(row['pixels'].split(), dtype='float32').reshape(48, 48)\n",
    "        img = Image.fromarray(pixels).convert(\"RGB\").resize((224, 224))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = int(row['emotion'])\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce18736f-a105-497f-9d14-da1fb6c7ca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load and augmentation\n",
    "df = pd.read_csv(FER_CSV_PATH)\n",
    "train_df = df[df['Usage'] == 'Training'].reset_index(drop=True)\n",
    "val_df = df[df['Usage'] == 'PublicTest'].reset_index(drop=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "train_dataset = FERDataset(train_df, transform)\n",
    "val_dataset = FERDataset(val_df, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3f69c0-2fe2-4ddc-a78e-c0bfe45ac336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model With Emotion Head Only\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, vit, num_classes=7, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.vit = vit\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.emotion_head = nn.Sequential(\n",
    "            nn.Linear(vit.config.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(pixel_values=x).last_hidden_state[:, 0]\n",
    "        x = self.dropout(x)\n",
    "        return self.emotion_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe21b66-3dc2-4d16-a567-57d164483f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained ViT\n",
    "vit = ViTModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in vit.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze last 2 transformer blocks\n",
    "for name, param in vit.named_parameters():\n",
    "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "model = EmotionClassifier(vit).to(device)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cc930f4-3f6a-4c97-ae98-13d11c41ba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|█████████████████████████████████████████████████████████████████████| 449/449 [08:42<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3498, Accuracy: 55.12%\n",
      "Validation Accuracy: 60.41%\n",
      "Saved improved model at Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|█████████████████████████████████████████████████████████████████████| 449/449 [08:02<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2016, Accuracy: 62.91%\n",
      "Validation Accuracy: 63.58%\n",
      "Saved improved model at Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|█████████████████████████████████████████████████████████████████████| 449/449 [07:56<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1344, Accuracy: 66.34%\n",
      "Validation Accuracy: 65.51%\n",
      "Saved improved model at Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|█████████████████████████████████████████████████████████████████████| 449/449 [07:57<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0767, Accuracy: 69.61%\n",
      "Validation Accuracy: 66.20%\n",
      "Saved improved model at Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|█████████████████████████████████████████████████████████████████████| 449/449 [07:58<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0219, Accuracy: 72.48%\n",
      "Validation Accuracy: 65.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|█████████████████████████████████████████████████████████████████████| 449/449 [08:00<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9627, Accuracy: 75.93%\n",
      "Validation Accuracy: 67.12%\n",
      "Saved improved model at Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|█████████████████████████████████████████████████████████████████████| 449/449 [08:07<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9102, Accuracy: 78.15%\n",
      "Validation Accuracy: 66.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|█████████████████████████████████████████████████████████████████████| 449/449 [08:06<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8596, Accuracy: 81.01%\n",
      "Validation Accuracy: 67.18%\n",
      "Saved improved model at Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|█████████████████████████████████████████████████████████████████████| 449/449 [08:08<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8033, Accuracy: 84.06%\n",
      "Validation Accuracy: 66.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|████████████████████████████████████████████████████████████████████| 449/449 [08:15<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7623, Accuracy: 85.91%\n",
      "Validation Accuracy: 67.43%\n",
      "Saved improved model at Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30:   9%|██████▎                                                              | 41/449 [00:46<07:39,  1.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     loss.backward()\n\u001b[32m     18\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     correct += (outputs.argmax(\u001b[32m1\u001b[39m) == labels).sum().item()\n\u001b[32m     23\u001b[39m train_loss = total_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "train_losses, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    acc = 100 * correct / len(train_dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * val_correct / len(val_dataset)\n",
    "    val_accuracies.append(val_acc)\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"Saved improved model at Epoch {epoch+1}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588023a-4e82-4ac7-9175-dc5c6b27edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric')\n",
    "plt.title('Training Loss and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4eb3ed-ebe1-4991-81a8-b6d8fb108492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef2fd7-3609-4b8b-8830-b7fd6263fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "all_preds, all_labels = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=EMOTION_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cbf8a4-9a1e-49f1-8204-88f3ed0a2b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=EMOTION_CLASSES)\n",
    "disp.plot(xticks_rotation=45, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be04d1-4ed3-4415-b6c6-4e41db390470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predictions\n",
    "def show_predictions(model, dataset, num=6):\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(1, num, figsize=(15, 3))\n",
    "    for i in range(num):\n",
    "        img, label = dataset[i]\n",
    "        with torch.no_grad():\n",
    "            pred = model(img.unsqueeze(0).to(device)).argmax(1).item()\n",
    "        axes[i].imshow(img.permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "        axes[i].set_title(f\"GT: {EMOTION_CLASSES[label]}\\nPred: {EMOTION_CLASSES[pred]}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_predictions(model, val_dataset)\n",
    "\n",
    "print(f\"Training complete. Best model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9d896-a7f6-4248-b3bc-8d6a45796f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
