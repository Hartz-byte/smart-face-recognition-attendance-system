{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "610d5486-bd95-43ec-8a73-afd9e024874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import ViTModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15c491b5-0921-46bd-9c66-caa53145c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "FACE_DIR = \"../registered_faces\"\n",
    "EMOTION_DIR = \"../emotions_data\"\n",
    "SPOOF_DIR = \"../spoof_datasets/spoof\"\n",
    "SAVE_PATH = \"./saved_model/triple_head_vit.pth\"\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "LR = 1e-4\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f7346c-53ba-4893-afa5-4930f80ab757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "emotion_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4c156f8-4c10-471d-ba13-797f28149a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "face_dataset = datasets.ImageFolder(FACE_DIR, transform=base_transform)\n",
    "emotion_dataset = datasets.ImageFolder(EMOTION_DIR, transform=emotion_transform)\n",
    "spoof_dataset = datasets.ImageFolder(SPOOF_DIR, transform=base_transform)\n",
    "\n",
    "# Split emotion & spoof into train/val\n",
    "val_len = int(len(emotion_dataset) * VAL_SPLIT)\n",
    "emotion_train, emotion_val = random_split(emotion_dataset, [len(emotion_dataset)-val_len, val_len])\n",
    "\n",
    "val_len_spf = int(len(spoof_dataset) * VAL_SPLIT)\n",
    "spoof_train, spoof_val = random_split(spoof_dataset, [len(spoof_dataset)-val_len_spf, val_len_spf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b3807a2-222c-48ef-8156-de1959e4addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders\n",
    "face_loader = DataLoader(face_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "emotion_loader = DataLoader(emotion_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "spoof_loader = DataLoader(spoof_train, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf45b6d4-1b05-4cbc-bf3f-d4937c02fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "emotion_val_loader = DataLoader(emotion_val, batch_size=BATCH_SIZE)\n",
    "spoof_val_loader = DataLoader(spoof_val, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96a85e83-c860-4cee-959e-352e2ad5436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class TripleHeadViT(nn.Module):\n",
    "    def __init__(self, vit, face_classes, emotion_classes):\n",
    "        super().__init__()\n",
    "        self.vit = vit\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.face_head = nn.Linear(vit.config.hidden_size, face_classes)\n",
    "        self.emotion_head = nn.Linear(vit.config.hidden_size, emotion_classes)\n",
    "        self.spoof_head = nn.Linear(vit.config.hidden_size, 1)  # Binary class\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.vit(pixel_values=x).last_hidden_state[:, 0]\n",
    "        features = self.dropout(features)\n",
    "        return self.face_head(features), self.emotion_head(features), self.spoof_head(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1c86fca-e78a-4de8-b7f9-f1d8814e11c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "for name, param in vit.named_parameters():\n",
    "    if \"encoder.layer.11\" not in name and \"encoder.layer.10\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model = TripleHeadViT(vit, len(face_dataset.classes), len(emotion_dataset.classes)).to(device)\n",
    "\n",
    "# Losses\n",
    "face_criterion = nn.CrossEntropyLoss()\n",
    "emotion_criterion = nn.CrossEntropyLoss()\n",
    "spoof_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baa3383c-fe88-4237-b766-4793790128aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|█████████████████████████████████████████████████████████████████████████| 8/8 [00:12<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 3.5566 | Face Acc (Train): 42.19% | Emotion Acc (Train): 28.12% | Val: 25.78% | Spoof Acc (Train): 93.75% | Val: 93.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|█████████████████████████████████████████████████████████████████████████| 8/8 [00:10<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Loss: 2.6630 | Face Acc (Train): 82.81% | Emotion Acc (Train): 20.31% | Val: 29.29% | Spoof Acc (Train): 96.88% | Val: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|█████████████████████████████████████████████████████████████████████████| 8/8 [00:11<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Loss: 2.1749 | Face Acc (Train): 90.62% | Emotion Acc (Train): 34.38% | Val: 31.84% | Spoof Acc (Train): 100.00% | Val: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|█████████████████████████████████████████████████████████████████████████| 8/8 [00:10<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Loss: 2.2029 | Face Acc (Train): 92.19% | Emotion Acc (Train): 23.44% | Val: 35.15% | Spoof Acc (Train): 100.00% | Val: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|█████████████████████████████████████████████████████████████████████████| 8/8 [00:09<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Loss: 1.7408 | Face Acc (Train): 96.88% | Emotion Acc (Train): 45.31% | Val: 36.30% | Spoof Acc (Train): 100.00% | Val: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|█████████████████████████████████████████████████████████████████████████| 8/8 [00:10<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Loss: 1.9947 | Face Acc (Train): 98.44% | Emotion Acc (Train): 25.00% | Val: 39.36% | Spoof Acc (Train): 100.00% | Val: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|█████████████████████████████████████████████████████████████████████████| 8/8 [00:10<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Loss: 1.8334 | Face Acc (Train): 98.44% | Emotion Acc (Train): 34.38% | Val: 40.39% | Spoof Acc (Train): 100.00% | Val: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|█████████████████████████████████████████████████████████████████████████| 8/8 [00:09<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Loss: 1.7481 | Face Acc (Train): 100.00% | Emotion Acc (Train): 35.94% | Val: 42.27% | Spoof Acc (Train): 100.00% | Val: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|█████████████████████████████████████████████████████████████████████████| 8/8 [00:09<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Loss: 1.4817 | Face Acc (Train): 100.00% | Emotion Acc (Train): 45.31% | Val: 43.11% | Spoof Acc (Train): 100.00% | Val: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|████████████████████████████████████████████████████████████████████████| 8/8 [00:09<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Loss: 1.4538 | Face Acc (Train): 100.00% | Emotion Acc (Train): 50.00% | Val: 44.42% | Spoof Acc (Train): 100.00% | Val: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    face_correct, emotion_correct, spoof_correct = 0, 0, 0\n",
    "    face_total, emotion_total, spoof_total = 0, 0, 0\n",
    "\n",
    "    face_iter = iter(face_loader)\n",
    "    emotion_iter = iter(emotion_loader)\n",
    "    spoof_iter = iter(spoof_loader)\n",
    "\n",
    "    steps = min(len(face_iter), len(emotion_iter), len(spoof_iter))\n",
    "\n",
    "    for _ in tqdm(range(steps), desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        x_face, y_face = next(face_iter)\n",
    "        x_emotion, y_emotion = next(emotion_iter)\n",
    "        x_spoof, y_spoof = next(spoof_iter)\n",
    "\n",
    "        x_face, y_face = x_face.to(device), y_face.to(device)\n",
    "        x_emotion, y_emotion = x_emotion.to(device), y_emotion.to(device)\n",
    "        x_spoof, y_spoof = x_spoof.to(device), y_spoof.to(device).float().unsqueeze(1)\n",
    "\n",
    "        x = torch.cat([x_face, x_emotion, x_spoof], dim=0)\n",
    "        optimizer.zero_grad()\n",
    "        face_logits, emotion_logits, spoof_logits = model(x)\n",
    "\n",
    "        face_loss = face_criterion(face_logits[:len(y_face)], y_face)\n",
    "        emotion_loss = emotion_criterion(emotion_logits[len(y_face):len(y_face)+len(y_emotion)], y_emotion)\n",
    "        spoof_loss = spoof_criterion(spoof_logits[-len(y_spoof):], y_spoof)\n",
    "        loss = face_loss + emotion_loss + spoof_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Accuracy\n",
    "        face_preds = face_logits[:len(y_face)].argmax(1)\n",
    "        face_correct += (face_preds == y_face).sum().item()\n",
    "        face_total += len(y_face)\n",
    "\n",
    "        emotion_preds = emotion_logits[len(y_face):len(y_face)+len(y_emotion)].argmax(1)\n",
    "        emotion_correct += (emotion_preds == y_emotion).sum().item()\n",
    "        emotion_total += len(y_emotion)\n",
    "\n",
    "        spoof_preds = torch.sigmoid(spoof_logits[-len(y_spoof):]) > 0.5\n",
    "        spoof_correct += (spoof_preds.squeeze().int() == y_spoof.squeeze().int()).sum().item()\n",
    "        spoof_total += len(y_spoof)\n",
    "\n",
    "    # Calculate train accuracies\n",
    "    train_face_acc = 100 * face_correct / face_total\n",
    "    train_emotion_acc = 100 * emotion_correct / emotion_total\n",
    "    train_spoof_acc = 100 * spoof_correct / spoof_total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_emotion_correct, val_emotion_total = 0, 0\n",
    "    val_spoof_correct, val_spoof_total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in emotion_val_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            _, emotion_logits, _ = model(x_val)\n",
    "            preds = emotion_logits.argmax(1)\n",
    "            val_emotion_correct += (preds == y_val).sum().item()\n",
    "            val_emotion_total += len(y_val)\n",
    "\n",
    "        for x_val, y_val in spoof_val_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device).float().unsqueeze(1)\n",
    "            _, _, spoof_logits = model(x_val)\n",
    "            preds = (torch.sigmoid(spoof_logits) > 0.5).int()\n",
    "            val_spoof_correct += (preds == y_val.int()).sum().item()\n",
    "            val_spoof_total += len(y_val)\n",
    "\n",
    "    val_emotion_acc = 100 * val_emotion_correct / val_emotion_total\n",
    "    val_spoof_acc = 100 * val_spoof_correct / val_spoof_total\n",
    "\n",
    "    # Log all results\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss/steps:.4f} | \"\n",
    "          f\"Face Acc (Train): {train_face_acc:.2f}% | \"\n",
    "          f\"Emotion Acc (Train): {train_emotion_acc:.2f}% | Val: {val_emotion_acc:.2f}% | \"\n",
    "          f\"Spoof Acc (Train): {train_spoof_acc:.2f}% | Val: {val_spoof_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc8b932e-0b7d-40cc-b430-bea0ff88e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triple-head model saved at ./saved_model/triple_head_vit.pth\n"
     ]
    }
   ],
   "source": [
    "# SAVE\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"face_classes\": face_dataset.classes,\n",
    "    \"emotion_classes\": emotion_dataset.classes\n",
    "}, SAVE_PATH)\n",
    "\n",
    "print(f\"Triple-head model saved at {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d544a5-e893-4ef3-a571-1788354b972b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
