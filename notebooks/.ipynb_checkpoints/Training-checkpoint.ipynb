{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c5ed6fe-96b5-4cc6-b498-5d42e4407290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/ML/Projects/smart-face-recognition-attendance-system/facenet-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-09 08:16:18.738600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752048979.085706    2131 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752048979.181875    2131 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752048980.035431    2131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752048980.035562    2131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752048980.035567    2131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752048980.035571    2131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-09 08:16:20.161257: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTModel, ViTImageProcessor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c70619c-71b9-473d-81a9-527902f2be15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration & device\n",
    "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
    "DATA_DIR = \"../registered_faces\"\n",
    "EMBEDDINGS_FILE = \"face_embeddings.npy\"\n",
    "NAMES_FILE = \"face_names.npy\"\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a093dfd5-ad16-4286-90c9-695cd1b6d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Harsh', 'Mummy', 'Papa']\n"
     ]
    }
   ],
   "source": [
    "# Load & preprocess dataset\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=DATA_DIR, transform=transform)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Classes: {dataset.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f9f4a5-d39f-48a2-9821-762d3014fbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Build model (ViT + classifier head)\n",
    "vit = ViTModel.from_pretrained(MODEL_NAME)\n",
    "class FaceClassifier(nn.Module):\n",
    "    def __init__(self, vit, num_classes):\n",
    "        super().__init__()\n",
    "        self.vit = vit\n",
    "        self.classifier = nn.Linear(vit.config.hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        outputs = self.vit(pixel_values=x)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "model = FaceClassifier(vit, num_classes=len(dataset.classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41075918-bfe5-4011-a7a0-60d411a4c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:10<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 0.7869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:07<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5] Loss: 0.0535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:07<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5] Loss: 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:08<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5] Loss: 0.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:05<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for imgs, labels in tqdm(loader):\n",
    "        inputs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {epoch_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "209fad34-f752-4963-804f-7076a26c268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings & names to 'saved_embeddings' folder.\n"
     ]
    }
   ],
   "source": [
    "# Extract and save embeddings\n",
    "model.eval()\n",
    "embeddings = []\n",
    "names = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in loader:\n",
    "        inputs = imgs.to(device)\n",
    "        outputs = model.vit(pixel_values=inputs)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        emb = pooled.cpu().numpy()\n",
    "        embeddings.append(emb)\n",
    "        for label in labels:\n",
    "            names.append(dataset.classes[label])\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "np.save(\"saved_embeddings/\" + EMBEDDINGS_FILE, embeddings)\n",
    "np.save(\"saved_embeddings/\" + NAMES_FILE, np.array(names))\n",
    "\n",
    "print(\"Saved embeddings & names to 'saved_embeddings' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "997dcde6-fe11-44e0-b62e-332616fba299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing images in folder: Harsh ===\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "\n",
      "=== Testing images in folder: Mummy ===\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "\n",
      "=== Testing images in folder: Papa ===\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n"
     ]
    }
   ],
   "source": [
    "# Test on a single image\n",
    "def recognize(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.vit(pixel_values=img_tensor)\n",
    "        emb = output.last_hidden_state[:,0].cpu().numpy()\n",
    "    loaded_embs = np.load(\"saved_embeddings/\" + EMBEDDINGS_FILE)\n",
    "    loaded_names = np.load(\"saved_embeddings/\" + NAMES_FILE)\n",
    "    sims = cosine_similarity(emb, loaded_embs)[0]\n",
    "    best_idx = np.argmax(sims)\n",
    "    print(f\"Predicted: {loaded_names[best_idx]} (score: {sims[best_idx]:.2f})\")\n",
    "\n",
    "# Test\n",
    "root_folder = \"../registered_faces\"\n",
    "for person in os.listdir(root_folder):\n",
    "    person_folder = os.path.join(root_folder, person)\n",
    "    if not os.path.isdir(person_folder):\n",
    "        continue\n",
    "    print(f\"\\n=== Testing images in folder: {person} ===\")\n",
    "    for img_file in os.listdir(person_folder):\n",
    "        img_path = os.path.join(person_folder, img_file)\n",
    "        recognize(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c91c31b-88eb-4e8b-acb7-40a0e30a1721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to saved_model/vit_face_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "MODEL_SAVE_PATH = \"saved_model/vit_face_classifier.pth\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'class_to_idx': dataset.class_to_idx\n",
    "}, MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5b664-1f6c-454d-b661-7e7c58a972ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
