{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b8a0ef-8177-412b-8a28-bff995e9c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c5ed6fe-96b5-4cc6-b498-5d42e4407290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752061028.475797    2487 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752061028.597678    2487 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752061029.652998    2487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752061029.653064    2487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752061029.653070    2487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752061029.653074    2487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel\n",
    "from facenet_pytorch import MTCNN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from collections import deque, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c70619c-71b9-473d-81a9-527902f2be15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
    "DATA_DIR = \"../registered_faces\"\n",
    "EMBEDDINGS_FILE = \"face_embeddings.npy\"\n",
    "NAMES_FILE = \"face_names.npy\"\n",
    "MODEL_SAVE_PATH = \"saved_model/vit_face_classifier.pth\"\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "SMOOTHING_WINDOW = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a093dfd5-ad16-4286-90c9-695cd1b6d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MTCNN\n",
    "mtcnn = MTCNN(image_size=224, margin=20)\n",
    "\n",
    "# Data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Custom Dataset with face detection\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "        self.transform = transform\n",
    "        classes = sorted(os.listdir(root_dir))\n",
    "        for idx, class_name in enumerate(classes):\n",
    "            self.class_to_idx[class_name] = idx\n",
    "            class_folder = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for img_name in os.listdir(class_folder):\n",
    "                img_path = os.path.join(class_folder, img_name)\n",
    "                self.samples.append(img_path)\n",
    "                self.labels.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        face = mtcnn(img)\n",
    "        if face is None:\n",
    "            # fallback: return blank tensor\n",
    "            face = torch.zeros(3, 224, 224)\n",
    "        else:\n",
    "            face = transforms.ToPILImage()(face)\n",
    "            face = self.transform(face)\n",
    "        return face, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d324afe2-6cb5-4bed-b22b-24b2fc35c366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Harsh', 'Mummy', 'Papa']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = FaceDataset(DATA_DIR, transform)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "idx_to_class = {v:k for k,v in dataset.class_to_idx.items()}\n",
    "print(f\"Classes: {list(dataset.class_to_idx.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f9f4a5-d39f-48a2-9821-762d3014fbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "vit = ViTModel.from_pretrained(MODEL_NAME)\n",
    "class FaceClassifier(nn.Module):\n",
    "    def __init__(self, vit, num_classes):\n",
    "        super().__init__()\n",
    "        self.vit = vit\n",
    "        self.classifier = nn.Linear(vit.config.hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        outputs = self.vit(pixel_values=x)\n",
    "        pooled = outputs.last_hidden_state[:,0]\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "model = FaceClassifier(vit, num_classes=len(dataset.class_to_idx)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41075918-bfe5-4011-a7a0-60d411a4c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8/8 [00:51<00:00,  6.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 1.3365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8/8 [00:50<00:00,  6.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5] Loss: 0.4654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8/8 [00:50<00:00,  6.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5] Loss: 0.2338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8/8 [00:47<00:00,  5.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5] Loss: 0.0956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8/8 [00:53<00:00,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] Loss: 0.0439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for imgs, labels in tqdm(loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {epoch_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "209fad34-f752-4963-804f-7076a26c268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings & names.\n"
     ]
    }
   ],
   "source": [
    "# Extract embeddings from trained model\n",
    "model.eval()\n",
    "embeddings, names = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        pooled = model.vit(pixel_values=imgs).last_hidden_state[:,0]\n",
    "        embeddings.append(pooled.cpu().numpy())\n",
    "        for label in labels:\n",
    "            names.append(idx_to_class[label.item()])\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "np.save(\"saved_embeddings/\" + EMBEDDINGS_FILE, embeddings)\n",
    "np.save(\"saved_embeddings/\" + NAMES_FILE, np.array(names))\n",
    "print(\"Saved embeddings & names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "997dcde6-fe11-44e0-b62e-332616fba299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal smoothing with weighted scores\n",
    "recent_predictions = deque(maxlen=SMOOTHING_WINDOW)\n",
    "recent_scores = deque(maxlen=SMOOTHING_WINDOW)\n",
    "\n",
    "def recognize_with_smoothing(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    face = mtcnn(img)\n",
    "    if face is None:\n",
    "        print(\"No face detected.\")\n",
    "        return\n",
    "    face = transforms.ToPILImage()(face)\n",
    "    img_tensor = transform(face).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pooled = model.vit(pixel_values=img_tensor).last_hidden_state[:,0].cpu().numpy()\n",
    "    loaded_embs = np.load(\"saved_embeddings/\" + EMBEDDINGS_FILE)\n",
    "    loaded_names = np.load(\"saved_embeddings/\" + NAMES_FILE)\n",
    "    sims = cosine_similarity(pooled, loaded_embs)[0]\n",
    "    best_idx = np.argmax(sims)\n",
    "    predicted = loaded_names[best_idx]\n",
    "    recent_predictions.append(predicted)\n",
    "    recent_scores.append(sims[best_idx])\n",
    "    # weighted smoothing\n",
    "    weighted = {}\n",
    "    for name, score in zip(recent_predictions, recent_scores):\n",
    "        weighted[name] = weighted.get(name, 0) + score\n",
    "    most_common = max(weighted, key=weighted.get)\n",
    "    print(f\"Predicted: {predicted} (score: {sims[best_idx]:.2f}), Smoothed: {most_common}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05c56095-d80c-4954-9d69-6ed40ad56706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing images in folder: Harsh ---\n",
      "Predicted: Harsh (score: 0.83), Smoothed: Harsh\n",
      "Predicted: Harsh (score: 0.84), Smoothed: Harsh\n",
      "Predicted: Harsh (score: 0.93), Smoothed: Harsh\n",
      "Predicted: Harsh (score: 0.89), Smoothed: Harsh\n",
      "Predicted: Harsh (score: 0.90), Smoothed: Harsh\n",
      "Predicted: Harsh (score: 0.97), Smoothed: Harsh\n",
      "Predicted: Harsh (score: 0.84), Smoothed: Harsh\n",
      "Predicted: Harsh (score: 0.79), Smoothed: Harsh\n",
      "Predicted: Harsh (score: 0.88), Smoothed: Harsh\n",
      "Predicted: Harsh (score: 0.87), Smoothed: Harsh\n",
      "\n",
      "--- Testing images in folder: Mummy ---\n",
      "Predicted: Mummy (score: 0.91), Smoothed: Mummy\n",
      "Predicted: Mummy (score: 0.91), Smoothed: Mummy\n",
      "Predicted: Mummy (score: 0.95), Smoothed: Mummy\n",
      "Predicted: Mummy (score: 0.92), Smoothed: Mummy\n",
      "Predicted: Mummy (score: 0.88), Smoothed: Mummy\n",
      "Predicted: Mummy (score: 0.84), Smoothed: Mummy\n",
      "Predicted: Mummy (score: 0.81), Smoothed: Mummy\n",
      "Predicted: Mummy (score: 0.95), Smoothed: Mummy\n",
      "Predicted: Mummy (score: 0.81), Smoothed: Mummy\n",
      "Predicted: Mummy (score: 0.92), Smoothed: Mummy\n",
      "\n",
      "--- Testing images in folder: Papa ---\n",
      "Predicted: Papa (score: 0.88), Smoothed: Papa\n",
      "Predicted: Papa (score: 0.90), Smoothed: Papa\n",
      "Predicted: Papa (score: 0.86), Smoothed: Papa\n",
      "Predicted: Papa (score: 0.81), Smoothed: Papa\n",
      "Predicted: Papa (score: 0.96), Smoothed: Papa\n",
      "Predicted: Papa (score: 0.79), Smoothed: Papa\n",
      "Predicted: Papa (score: 0.90), Smoothed: Papa\n",
      "Predicted: Papa (score: 0.85), Smoothed: Papa\n",
      "Predicted: Papa (score: 0.92), Smoothed: Papa\n",
      "Predicted: Papa (score: 0.93), Smoothed: Papa\n"
     ]
    }
   ],
   "source": [
    "# Test all images\n",
    "root_folder = \"../registered_faces\"\n",
    "\n",
    "for person in os.listdir(root_folder):\n",
    "    person_folder = os.path.join(root_folder, person)\n",
    "    if not os.path.isdir(person_folder):\n",
    "        continue\n",
    "    print(f\"\\n--- Testing images in folder: {person} ---\")\n",
    "    recent_predictions.clear()\n",
    "    recent_scores.clear()\n",
    "    for img_file in os.listdir(person_folder):\n",
    "        img_path = os.path.join(person_folder, img_file)\n",
    "        recognize_with_smoothing(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c91c31b-88eb-4e8b-acb7-40a0e30a1721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to saved_model/vit_face_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'class_to_idx': dataset.class_to_idx\n",
    "}, MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5b664-1f6c-454d-b661-7e7c58a972ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
