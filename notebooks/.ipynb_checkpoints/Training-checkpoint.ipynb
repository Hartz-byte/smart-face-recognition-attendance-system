{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3834ab-1a81-4de7-bb4b-57319ce78a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25091f30-c1ce-4ff0-9da3-b09520c43d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752142944.183949    3227 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752142944.202482    3227 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752142944.309577    3227 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752142944.309635    3227 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752142944.309638    3227 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752142944.309640    3227 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTModel, ViTImageProcessor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53913ce-82b6-467c-9d9a-4c06539cc876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
    "DATA_DIR = \"../registered_faces\"\n",
    "EMBEDDINGS_DIR = \"../notebooks/saved_embeddings\"\n",
    "MODEL_SAVE_PATH = \"../notebooks/saved_model/vit_face_classifier.pth\"\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9896f2aa-a03e-4f1e-82be-03ab4f6b3a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Harsh', 'Mummy', 'Papa']\n"
     ]
    }
   ],
   "source": [
    "# Load & preprocess dataset\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=DATA_DIR, transform=transform)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Classes: {dataset.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a10c994-4370-4747-b846-e9d19ce43dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Build model (ViT + classifier head)\n",
    "vit = ViTModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class FaceClassifier(nn.Module):\n",
    "    def __init__(self, vit, num_classes):\n",
    "        super().__init__()\n",
    "        self.vit = vit\n",
    "        self.classifier = nn.Linear(vit.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.vit(pixel_values=x)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "model = FaceClassifier(vit, len(dataset.classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bbbcb52-1ec8-4703-b9fc-d1380212c18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:07<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.8335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.0662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:06<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:06<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:03<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for imgs, labels in tqdm(loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f00135-dab3-4ccf-84cc-62578d6d17a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model and embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Extract & save embeddings\n",
    "model.eval()\n",
    "embeddings, names = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in loader:\n",
    "        inputs = imgs.to(device)\n",
    "        outputs = model.vit(pixel_values=inputs)\n",
    "        pooled = outputs.last_hidden_state[:, 0].cpu().numpy()\n",
    "        embeddings.append(pooled)\n",
    "        names += [dataset.classes[label] for label in labels]\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "np.save(f\"{EMBEDDINGS_DIR}/face_embeddings.npy\", embeddings)\n",
    "np.save(f\"{EMBEDDINGS_DIR}/face_names.npy\", np.array(names))\n",
    "\n",
    "print(\"Saved model and embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11be82c1-dd4e-4e9b-949b-994144ea52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single image\n",
    "def recognize(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.vit(pixel_values=img_tensor)\n",
    "        emb = output.last_hidden_state[:,0].cpu().numpy()\n",
    "    loaded_embs = np.load(EMBEDDINGS_FILE)\n",
    "    loaded_names = np.load(NAMES_FILE)\n",
    "    sims = cosine_similarity(emb, loaded_embs)[0]\n",
    "    best_idx = np.argmax(sims)\n",
    "    \n",
    "    print(f\"Predicted: {loaded_names[best_idx]} (score: {sims[best_idx]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8ae2368-2517-4ad5-ac34-b37f796383c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing images in folder: Harsh ===\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "Predicted: Harsh (score: 1.00)\n",
      "\n",
      "=== Testing images in folder: Mummy ===\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "Predicted: Mummy (score: 1.00)\n",
      "\n",
      "=== Testing images in folder: Papa ===\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n",
      "Predicted: Papa (score: 1.00)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "root_folder = \"../registered_faces\"\n",
    "EMBEDDINGS_FILE = \"saved_embeddings/face_embeddings.npy\"\n",
    "NAMES_FILE = \"saved_embeddings/face_names.npy\"\n",
    "\n",
    "for person in os.listdir(root_folder):\n",
    "    person_folder = os.path.join(root_folder, person)\n",
    "    if not os.path.isdir(person_folder):\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n=== Testing images in folder: {person} ===\")\n",
    "    \n",
    "    for img_file in os.listdir(person_folder):\n",
    "        img_path = os.path.join(person_folder, img_file)\n",
    "        recognize(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "446dcf5a-7511-44f8-bcc0-cf70c823c1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../notebooks/saved_model/vit_face_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'class_to_idx': dataset.class_to_idx\n",
    "}, MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01259863-b99b-46d2-8a1e-9637475cbc42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
