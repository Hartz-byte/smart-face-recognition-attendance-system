{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c6fcb-3b02-42f8-9c7e-3a662cb258db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3e2bb-8379-4aa6-a392-e01e96719bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
    "DATA_ROOT = \"../emotions_data\"\n",
    "MODEL_SAVE_PATH = \"saved_model/vit_emotion_head.pth\"\n",
    "EMOTION_CLASSES = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "PATIENCE = 3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291cbdf-0635-4750-aa0f-1adc9a5cb54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class FERImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, emotion_classes, transform=None):\n",
    "        self.transform = transform\n",
    "        self.samples, self.labels = [], []\n",
    "        for idx, cls in enumerate(emotion_classes):\n",
    "            class_dir = os.path.join(root_dir, cls)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.samples.append(os.path.join(class_dir, img_name))\n",
    "                    self.labels.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((224, 224))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18736f-a105-497f-9d14-da1fb6c7ca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Datasets and Loaders\n",
    "train_dataset = FERImageDataset(os.path.join(DATA_ROOT, \"Training\"), EMOTION_CLASSES, transform)\n",
    "val_dataset = FERImageDataset(os.path.join(DATA_ROOT, \"PublicTest\"), EMOTION_CLASSES, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f69c0-2fe2-4ddc-a78e-c0bfe45ac336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, vit, num_classes=7, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.vit = vit\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.emotion_head = nn.Sequential(\n",
    "            nn.Linear(vit.config.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(pixel_values=x).last_hidden_state[:, 0]\n",
    "        x = self.dropout(x)\n",
    "        return self.emotion_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe21b66-3dc2-4d16-a567-57d164483f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ViT\n",
    "vit = ViTModel.from_pretrained(MODEL_NAME)\n",
    "for param in vit.parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in vit.named_parameters():\n",
    "    if any(f\"encoder.layer.{i}\" in name for i in [10, 11]):\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Class balancing\n",
    "class_counts = [train_dataset.labels.count(i) for i in range(7)]\n",
    "class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n",
    "class_weights = class_weights / class_weights.sum() * len(EMOTION_CLASSES)\n",
    "\n",
    "# Loss, optimizer, scheduler\n",
    "model = EmotionClassifier(vit).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc930f4-3f6a-4c97-ae98-13d11c41ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "train_losses, val_accuracies = [], []\n",
    "min_delta = 0.1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    acc = 100 * correct / len(train_dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * val_correct / len(val_dataset)\n",
    "    val_accuracies.append(val_acc)\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc + min_delta:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"Saved improved model at Epoch {epoch+1}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No significant improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered due to validation accuracy plateau.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588023a-4e82-4ac7-9175-dc5c6b27edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric')\n",
    "plt.title('Training Loss and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4eb3ed-ebe1-4991-81a8-b6d8fb108492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=EMOTION_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef2fd7-3609-4b8b-8830-b7fd6263fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=EMOTION_CLASSES)\n",
    "disp.plot(xticks_rotation=45, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cbf8a4-9a1e-49f1-8204-88f3ed0a2b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction samples\n",
    "def show_predictions(model, dataset, num=6):\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(1, num, figsize=(15, 3))\n",
    "    for i in range(num):\n",
    "        img, label = dataset[i]\n",
    "        with torch.no_grad():\n",
    "            pred = model(img.unsqueeze(0).to(device)).argmax(1).item()\n",
    "        axes[i].imshow(img.permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "        axes[i].set_title(f\"GT: {EMOTION_CLASSES[label]}\\nPred: {EMOTION_CLASSES[pred]}\",\n",
    "                          color='green' if label == pred else 'red')\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_predictions(model, val_dataset)\n",
    "\n",
    "print(f\"Training complete. Best model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9d896-a7f6-4248-b3bc-8d6a45796f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
