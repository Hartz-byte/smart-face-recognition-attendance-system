{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8cd541-cb08-40ff-9554-ec4a89d818e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/ML/Projects/smart-face-recognition-attendance-system/facenet-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-04 06:08:08.669502: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754287688.925176    2399 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754287689.001388    2399 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754287689.661944    2399 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754287689.661997    2399 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754287689.661999    2399 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754287689.662001    2399 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-04 06:08:09.753335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTModel\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# CONFIG\n",
    "MODEL_PATH = \"./saved_model/triple_head_vit.pth\"\n",
    "FEEDBACK_FACE_DIR = \"../feedback_frames/self_training/face\"\n",
    "FEEDBACK_EMOTION_DIR = \"../feedback_frames/self_training/emotion\"\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373d1029-5402-4981-b721-172d0975f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original class lists\n",
    "with open('./saved_model/face_classes.json', 'r') as f:\n",
    "    face_classes = json.load(f)\n",
    "with open('./saved_model/emotion_classes.json', 'r') as f:\n",
    "    emotion_classes = json.load(f)\n",
    "\n",
    "# Transforms\n",
    "feedback_face_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "feedback_emotion_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef17d71-38a7-4ebb-ac6c-3a10a75359b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedback Dataset\n",
    "class FeedbackDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dir, class_names, transform):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for cls in class_names:\n",
    "            cls_path = os.path.join(base_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                for img_name in os.listdir(cls_path):\n",
    "                    if img_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                        self.samples.append((os.path.join(cls_path, img_name), class_names.index(cls)))\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return self.transform(img), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78ce802e-3642-4f93-82aa-31806f9292df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoaders\n",
    "face_feedback = FeedbackDataset(FEEDBACK_FACE_DIR, face_classes, feedback_face_transform)\n",
    "emotion_feedback = FeedbackDataset(FEEDBACK_EMOTION_DIR, emotion_classes, feedback_emotion_transform)\n",
    "face_loader = DataLoader(face_feedback, batch_size=BATCH_SIZE)\n",
    "emotion_loader = DataLoader(emotion_feedback, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c8e8ad-81f2-4381-9fe8-d1824b76dee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TripleHeadViT(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (face_head): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (emotion_head): Linear(in_features=768, out_features=7, bias=True)\n",
       "  (spoof_head): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model definition\n",
    "class TripleHeadViT(nn.Module):\n",
    "    def __init__(self, vit, face_classes, emotion_classes):\n",
    "        super().__init__()\n",
    "        self.vit = vit\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.face_head = nn.Linear(vit.config.hidden_size, face_classes)\n",
    "        self.emotion_head = nn.Linear(vit.config.hidden_size, emotion_classes)\n",
    "        self.spoof_head = nn.Linear(vit.config.hidden_size, 1)\n",
    "    def forward(self, x):\n",
    "        features = self.vit(pixel_values=x).last_hidden_state[:, 0]\n",
    "        features = self.dropout(features)\n",
    "        return self.face_head(features), self.emotion_head(features), self.spoof_head(features)\n",
    "\n",
    "# Load model\n",
    "ckpt = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = TripleHeadViT(vit, len(face_classes), len(emotion_classes)).to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"], strict=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb87d91d-db0d-41e1-a5ed-adfe8486b674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on feedback dataset:\n",
      "  Face recognition accuracy:   100.00%\n",
      "  Emotion classification accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "def evaluate(loader, head):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            face_logits, emotion_logits, _ = model(imgs)\n",
    "            if head == 'face':\n",
    "                preds = face_logits.argmax(dim=1)\n",
    "            elif head == 'emotion':\n",
    "                preds = emotion_logits.argmax(dim=1)\n",
    "            else:\n",
    "                raise ValueError(\"Head must be 'face' or 'emotion'\")\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100.0 * correct / total if total > 0 else 0.0\n",
    "\n",
    "face_acc = evaluate(face_loader, 'face')\n",
    "emotion_acc = evaluate(emotion_loader, 'emotion')\n",
    "\n",
    "print(\"Evaluation on feedback dataset:\")\n",
    "print(f\"  Face recognition accuracy:   {face_acc:.2f}%\")\n",
    "print(f\"  Emotion classification accuracy: {emotion_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689c9b7-e420-4ac0-a78d-22427d84b98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
