{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f219c47e-69d4-401e-b918-3cb543e18b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e87dff5-e579-490d-a02c-720755730637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752517212.021468   10838 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752517212.028192   10838 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752517212.050792   10838 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752517212.050821   10838 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752517212.050823   10838 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752517212.050825   10838 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73115ad8-31ed-4978-a7cd-401096fbbf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "REGISTERED_FACE_DIR = \"../registered_faces\"\n",
    "FER_DIR = \"../emotions_data\"\n",
    "SAVE_PATH = \"/saved_model/dual_head_vit.pth\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-4\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5371503b-b775-47ce-9d4a-9f4084370b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers\n",
    "common_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "face_dataset = datasets.ImageFolder(REGISTERED_FACE_DIR, transform=common_transform)\n",
    "emotion_dataset = datasets.ImageFolder(FER_DIR, transform=common_transform)\n",
    "\n",
    "face_loader = DataLoader(face_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "emotion_loader = DataLoader(emotion_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbaceac-99d9-4b16-892b-96f72f9acd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "class DualHeadViT(nn.Module):\n",
    "    def __init__(self, vit, face_classes, emotion_classes):\n",
    "        super().__init__()\n",
    "        self.vit = vit\n",
    "        self.face_head = nn.Linear(vit.config.hidden_size, face_classes)\n",
    "        self.emotion_head = nn.Linear(vit.config.hidden_size, emotion_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.vit(pixel_values=x).last_hidden_state[:, 0]\n",
    "        face_out = self.face_head(features)\n",
    "        emotion_out = self.emotion_head(features)\n",
    "        return face_out, emotion_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50425525-4029-44fb-ba5a-462669daf339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load base ViT\n",
    "vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = DualHeadViT(vit, len(face_dataset.classes), len(emotion_dataset.classes)).to(device)\n",
    "\n",
    "# Loss + Optimizer\n",
    "face_criterion = nn.CrossEntropyLoss()\n",
    "emotion_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df58e239-d902-489f-b2cb-9684dc6d3f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████████████████████████████████████████████████████████████████| 22/22 [00:32<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.1971 | Face Acc: 85.23% | Emotion Acc: 27.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|███████████████████████████████████████████████████████████████████████| 22/22 [00:32<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.6087 | Face Acc: 100.00% | Emotion Acc: 39.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|███████████████████████████████████████████████████████████████████████| 22/22 [00:32<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.5030 | Face Acc: 100.00% | Emotion Acc: 44.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|███████████████████████████████████████████████████████████████████████| 22/22 [00:32<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4416 | Face Acc: 100.00% | Emotion Acc: 48.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|███████████████████████████████████████████████████████████████████████| 22/22 [00:33<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3399 | Face Acc: 100.00% | Emotion Acc: 49.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|███████████████████████████████████████████████████████████████████████| 22/22 [00:32<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3480 | Face Acc: 100.00% | Emotion Acc: 45.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|███████████████████████████████████████████████████████████████████████| 22/22 [00:32<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2136 | Face Acc: 100.00% | Emotion Acc: 48.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|███████████████████████████████████████████████████████████████████████| 22/22 [00:32<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2903 | Face Acc: 100.00% | Emotion Acc: 56.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|███████████████████████████████████████████████████████████████████████| 22/22 [00:32<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3278 | Face Acc: 100.00% | Emotion Acc: 46.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████████████████████████████████████████████████████████████████| 22/22 [00:32<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1824 | Face Acc: 100.00% | Emotion Acc: 60.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    face_correct, emotion_correct = 0, 0\n",
    "    face_total, emotion_total = 0, 0\n",
    "\n",
    "    face_iter = iter(face_loader)\n",
    "    emotion_iter = iter(emotion_loader)\n",
    "    steps = min(len(face_iter), len(emotion_iter))\n",
    "\n",
    "    for _ in tqdm(range(steps), desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        try:\n",
    "            x_face, y_face = next(face_iter)\n",
    "            x_emotion, y_emotion = next(emotion_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        x_face, y_face = x_face.to(device), y_face.to(device)\n",
    "        x_emotion, y_emotion = x_emotion.to(device), y_emotion.to(device)\n",
    "\n",
    "        # Combine batches\n",
    "        x = torch.cat([x_face, x_emotion], dim=0)\n",
    "        face_labels = torch.cat([y_face, torch.zeros_like(y_face)], dim=0)\n",
    "        emotion_labels = torch.cat([torch.zeros_like(y_emotion), y_emotion], dim=0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        face_logits, emotion_logits = model(x)\n",
    "\n",
    "        # Only compute loss on relevant parts\n",
    "        face_loss = face_criterion(face_logits[:len(y_face)], y_face)\n",
    "        emotion_loss = emotion_criterion(emotion_logits[len(y_face):], y_emotion)\n",
    "        loss = face_loss + emotion_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        face_preds = face_logits[:len(y_face)].argmax(1)\n",
    "        face_correct += (face_preds == y_face).sum().item()\n",
    "        face_total += len(y_face)\n",
    "\n",
    "        emotion_preds = emotion_logits[len(y_face):].argmax(1)\n",
    "        emotion_correct += (emotion_preds == y_emotion).sum().item()\n",
    "        emotion_total += len(y_emotion)\n",
    "\n",
    "    acc_face = 100 * face_correct / face_total\n",
    "    acc_emotion = 100 * emotion_correct / emotion_total\n",
    "\n",
    "    print(f\"Loss: {total_loss/steps:.4f} | Face Acc: {acc_face:.2f}% | Emotion Acc: {acc_emotion:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "812cdc30-fabc-44a6-aaf9-b8d3142a10cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./saved_model/dual_head_vit.pth\n"
     ]
    }
   ],
   "source": [
    "# SAVE\n",
    "SAVE_PATH = \"./saved_model/dual_head_vit.pth\"\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"face_classes\": face_dataset.classes,\n",
    "    \"emotion_classes\": emotion_dataset.classes\n",
    "}, SAVE_PATH)\n",
    "\n",
    "print(f\"Model saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa74ea5-50dd-4aff-bffe-c3532b12bcec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
