{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3834ab-1a81-4de7-bb4b-57319ce78a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25091f30-c1ce-4ff0-9da3-b09520c43d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752146344.498452    4411 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752146344.506680    4411 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752146344.537697    4411 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752146344.537731    4411 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752146344.537733    4411 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752146344.537735    4411 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTModel, ViTImageProcessor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53913ce-82b6-467c-9d9a-4c06539cc876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
    "DATA_DIR = \"../registered_faces\"\n",
    "EMBEDDINGS_DIR = \"../notebooks/saved_embeddings\"\n",
    "MODEL_SAVE_PATH = \"../notebooks/saved_model/vit_face_classifier.pth\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "FREEZE_BACKBONE = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9896f2aa-a03e-4f1e-82be-03ab4f6b3a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Harsh', 'Mummy', 'Papa']\n"
     ]
    }
   ],
   "source": [
    "# Load & preprocess dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset & Dataloader split\n",
    "full_dataset = datasets.ImageFolder(root=DATA_DIR, transform=transform)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Apply different transform to val set\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Classes: {full_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64741311-6c4b-4c77-a935-9224cdb4ad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Harsh', 'Mummy', 'Papa']\n"
     ]
    }
   ],
   "source": [
    "# Dataset & Dataloader split\n",
    "full_dataset = datasets.ImageFolder(root=DATA_DIR, transform=transform)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Classes: {full_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a10c994-4370-4747-b846-e9d19ce43dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# # Build model (ViT + classifier head)\n",
    "# vit = ViTModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# class FaceClassifier(nn.Module):\n",
    "#     def __init__(self, vit, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.vit = vit\n",
    "#         self.classifier = nn.Linear(vit.config.hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         outputs = self.vit(pixel_values=x)\n",
    "#         pooled = outputs.last_hidden_state[:, 0]\n",
    "#         return self.classifier(pooled)\n",
    "\n",
    "# model = FaceClassifier(vit, len(dataset.classes)).to(device)\n",
    "\n",
    "# Build model\n",
    "vit = ViTModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if FREEZE_BACKBONE:\n",
    "    for param in vit.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "class FaceClassifier(nn.Module):\n",
    "    def __init__(self, vit, num_classes):\n",
    "        super().__init__()\n",
    "        self.vit = vit\n",
    "        self.classifier = nn.Linear(vit.config.hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.vit(pixel_values=x).last_hidden_state[:, 0]\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = FaceClassifier(vit, len(full_dataset.classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bbbcb52-1ec8-4703-b9fc-d1380212c18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [00:05<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.1378 | Val Loss: 1.1030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [00:02<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.9206 | Val Loss: 1.0423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [00:05<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.7947 | Val Loss: 0.9679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [00:05<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.6960 | Val Loss: 0.8666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [00:05<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.6147 | Val Loss: 0.7975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [00:05<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.5421 | Val Loss: 0.7291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [00:02<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.4823 | Val Loss: 0.6727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [00:05<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.4315 | Val Loss: 0.6278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [00:05<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.3871 | Val Loss: 0.5819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [00:05<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.3476 | Val Loss: 0.5494\n"
     ]
    }
   ],
   "source": [
    "# # Training loop\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(EPOCHS):\n",
    "#     total_loss = 0\n",
    "#     for imgs, labels in tqdm(loader):\n",
    "#         imgs, labels = imgs.to(device), labels.to(device)\n",
    "#         outputs = model(imgs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f00135-dab3-4ccf-84cc-62578d6d17a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved face embeddings and names.\n"
     ]
    }
   ],
   "source": [
    "# Extract & save embeddings\n",
    "model.eval()\n",
    "embeddings, names = [], []\n",
    "\n",
    "loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        outputs = model.vit(pixel_values=imgs)\n",
    "        pooled = outputs.last_hidden_state[:, 0].cpu().numpy()\n",
    "        embeddings.append(pooled)\n",
    "        names += [full_dataset.classes[label] for label in labels]\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "np.save(os.path.join(EMBEDDINGS_DIR, \"face_embeddings.npy\"), embeddings)\n",
    "np.save(os.path.join(EMBEDDINGS_DIR, \"face_names.npy\"), np.array(names))\n",
    "\n",
    "print(\"Saved face embeddings and names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11be82c1-dd4e-4e9b-949b-994144ea52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single image\n",
    "EMBEDDINGS_FILE = os.path.join(EMBEDDINGS_DIR, \"face_embeddings.npy\")\n",
    "NAMES_FILE = os.path.join(EMBEDDINGS_DIR, \"face_names.npy\")\n",
    "\n",
    "def recognize(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.vit(pixel_values=img_tensor)\n",
    "        emb = output.last_hidden_state[:,0].cpu().numpy()\n",
    "    loaded_embs = np.load(EMBEDDINGS_FILE)\n",
    "    loaded_names = np.load(NAMES_FILE)\n",
    "    sims = cosine_similarity(emb, loaded_embs)[0]\n",
    "    best_idx = np.argmax(sims)\n",
    "    \n",
    "    print(f\"Predicted: {loaded_names[best_idx]} (score: {sims[best_idx]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8ae2368-2517-4ad5-ac34-b37f796383c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing images in folder: Harsh ---\n",
      "Predicted: Harsh (score: 0.71)\n",
      "Predicted: Harsh (score: 0.76)\n",
      "Predicted: Harsh (score: 0.77)\n",
      "Predicted: Harsh (score: 0.65)\n",
      "Predicted: Harsh (score: 0.89)\n",
      "Predicted: Harsh (score: 0.73)\n",
      "Predicted: Harsh (score: 0.72)\n",
      "Predicted: Harsh (score: 0.65)\n",
      "Predicted: Harsh (score: 0.82)\n",
      "Predicted: Harsh (score: 0.67)\n",
      "Predicted: Harsh (score: 0.80)\n",
      "Predicted: Harsh (score: 0.85)\n",
      "Predicted: Harsh (score: 0.65)\n",
      "Predicted: Harsh (score: 0.77)\n",
      "Predicted: Harsh (score: 0.86)\n",
      "Predicted: Harsh (score: 0.76)\n",
      "Predicted: Harsh (score: 0.75)\n",
      "Predicted: Harsh (score: 0.83)\n",
      "Predicted: Harsh (score: 0.80)\n",
      "Predicted: Harsh (score: 0.64)\n",
      "Predicted: Harsh (score: 0.72)\n",
      "Predicted: Harsh (score: 0.83)\n",
      "Predicted: Harsh (score: 0.70)\n",
      "Predicted: Harsh (score: 0.85)\n",
      "Predicted: Harsh (score: 0.88)\n",
      "Predicted: Harsh (score: 0.73)\n",
      "Predicted: Harsh (score: 0.68)\n",
      "Predicted: Harsh (score: 0.90)\n",
      "Predicted: Harsh (score: 0.78)\n",
      "Predicted: Harsh (score: 0.91)\n",
      "Predicted: Harsh (score: 0.86)\n",
      "Predicted: Harsh (score: 0.77)\n",
      "Predicted: Harsh (score: 0.81)\n",
      "Predicted: Harsh (score: 0.86)\n",
      "Predicted: Harsh (score: 0.85)\n",
      "Predicted: Harsh (score: 0.74)\n",
      "\n",
      "--- Testing images in folder: Mummy ---\n",
      "Predicted: Mummy (score: 0.78)\n",
      "Predicted: Mummy (score: 0.76)\n",
      "Predicted: Mummy (score: 0.73)\n",
      "Predicted: Mummy (score: 0.80)\n",
      "Predicted: Mummy (score: 0.91)\n",
      "Predicted: Mummy (score: 0.87)\n",
      "Predicted: Mummy (score: 0.71)\n",
      "Predicted: Mummy (score: 0.74)\n",
      "Predicted: Mummy (score: 0.86)\n",
      "Predicted: Mummy (score: 0.75)\n",
      "Predicted: Mummy (score: 0.71)\n",
      "Predicted: Mummy (score: 0.79)\n",
      "Predicted: Mummy (score: 0.71)\n",
      "Predicted: Mummy (score: 0.86)\n",
      "Predicted: Mummy (score: 0.70)\n",
      "Predicted: Mummy (score: 0.87)\n",
      "Predicted: Mummy (score: 0.76)\n",
      "Predicted: Mummy (score: 0.77)\n",
      "Predicted: Mummy (score: 0.80)\n",
      "Predicted: Mummy (score: 0.88)\n",
      "Predicted: Mummy (score: 0.79)\n",
      "Predicted: Mummy (score: 0.74)\n",
      "Predicted: Mummy (score: 0.76)\n",
      "Predicted: Mummy (score: 0.81)\n",
      "Predicted: Mummy (score: 0.75)\n",
      "Predicted: Mummy (score: 0.80)\n",
      "Predicted: Mummy (score: 0.80)\n",
      "Predicted: Mummy (score: 0.72)\n",
      "\n",
      "--- Testing images in folder: Papa ---\n",
      "Predicted: Papa (score: 0.82)\n",
      "Predicted: Papa (score: 0.69)\n",
      "Predicted: Papa (score: 0.68)\n",
      "Predicted: Papa (score: 0.74)\n",
      "Predicted: Papa (score: 0.82)\n",
      "Predicted: Papa (score: 0.72)\n",
      "Predicted: Papa (score: 0.74)\n",
      "Predicted: Papa (score: 0.74)\n",
      "Predicted: Papa (score: 0.68)\n",
      "Predicted: Papa (score: 0.82)\n",
      "Predicted: Papa (score: 0.72)\n",
      "Predicted: Papa (score: 0.85)\n",
      "Predicted: Papa (score: 0.69)\n",
      "Predicted: Papa (score: 0.71)\n",
      "Predicted: Papa (score: 0.74)\n",
      "Predicted: Papa (score: 0.71)\n",
      "Predicted: Papa (score: 0.74)\n",
      "Predicted: Papa (score: 0.67)\n",
      "Predicted: Papa (score: 0.88)\n",
      "Predicted: Papa (score: 0.74)\n",
      "Predicted: Papa (score: 0.68)\n",
      "Predicted: Papa (score: 0.76)\n",
      "Predicted: Papa (score: 0.78)\n",
      "Predicted: Papa (score: 0.85)\n",
      "Predicted: Papa (score: 0.86)\n",
      "Predicted: Papa (score: 0.80)\n",
      "Predicted: Papa (score: 0.85)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "root_folder = \"../registered_faces\"\n",
    "EMBEDDINGS_FILE = \"saved_embeddings/face_embeddings.npy\"\n",
    "NAMES_FILE = \"saved_embeddings/face_names.npy\"\n",
    "\n",
    "for person in os.listdir(root_folder):\n",
    "    person_folder = os.path.join(root_folder, person)\n",
    "    if not os.path.isdir(person_folder):\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n--- Testing images in folder: {person} ---\")\n",
    "    \n",
    "    for img_file in os.listdir(person_folder):\n",
    "        img_path = os.path.join(person_folder, img_file)\n",
    "        recognize(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446dcf5a-7511-44f8-bcc0-cf70c823c1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../notebooks/saved_model/vit_face_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'class_to_idx': full_dataset.class_to_idx\n",
    "}, MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01259863-b99b-46d2-8a1e-9637475cbc42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
